{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39f5ca5",
   "metadata": {},
   "source": [
    "## Logistic Regression Explained\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Logistic regression is a statistical model used for predicting binary outcomes, like whether an email is spam or not spam, based on given features.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Binary Outcome**: Logistic regression predicts outcomes that are binary, meaning they have two possible values (e.g., 0 or 1, yes or no).\n",
    "\n",
    "2. **Linear Relationship**: It assumes a linear relationship between the input features and the log-odds of the outcome. The log-odds are transformed into probabilities using a special function.\n",
    "\n",
    "3. **Sigmoid Function**: Logistic regression uses the sigmoid function to map the predicted values to probabilities. The sigmoid function looks like this:\n",
    "\n",
    "   $$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "   Here, \\( z \\) is a linear combination of the input features and their corresponding coefficients.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- **Linearity**: It assumes that the relationship between the input features and the log-odds of the outcome is linear.\n",
    "  \n",
    "- **Independence of Errors**: The errors made by the model are assumed to be independent of each other.\n",
    "\n",
    "- **No Multicollinearity**: The input features should not be highly correlated with each other.\n",
    "\n",
    "### Equations\n",
    "\n",
    "- **Log-Odds**: The model predicts the log-odds of the outcome being 1 as a linear function of the input features:\n",
    "\n",
    "   $$\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n$$\n",
    "\n",
    "   Here, \\( p \\) is the probability of the outcome being 1.\n",
    "\n",
    "- **Probability Prediction**: Using the sigmoid function, the probability of the outcome being 1 is:\n",
    "\n",
    "   $$p(y=1 \\mid x) = \\sigma(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n)$$\n",
    "\n",
    "   And \\( p(y=0 \\mid x) = 1 - p(y=1 \\mid x) \\).\n",
    "\n",
    "### Learning from Data\n",
    "\n",
    "- **Parameter Estimation**: Logistic regression estimates the parameters  ( $ \\beta $ ) using maximum likelihood estimation (MLE). It finds the parameters that maximize the likelihood of observing the data given the model.\n",
    "\n",
    "- **Gradient Descent**: To optimize the parameters, logistic regression often uses gradient descent. Gradient descent adjusts the parameters iteratively to minimize the difference between predicted probabilities and actual outcomes.\n",
    "\n",
    "### Making Predictions\n",
    "\n",
    "- Once trained, the logistic regression model can predict the probability of a new data point belonging to a certain class (e.g., spam or not spam).\n",
    "  \n",
    "- It classifies the outcome based on a chosen threshold (typically 0.5). If the predicted probability is >= 0.5, it predicts class 1; otherwise, it predicts class 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04a282",
   "metadata": {},
   "source": [
    "# Function to load data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e10650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Skip header row\n",
    "        data = [[float(x) for x in row] for row in reader]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c57859d",
   "metadata": {},
   "source": [
    "# Function to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f9b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(data):\n",
    "    X = np.array([row[:-1] for row in data])\n",
    "    y = np.array([row[-1] for row in data])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c5810",
   "metadata": {},
   "source": [
    "# Feature scaling using standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d6de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature_scaling(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_scaled = (X - mean) / std\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9dff18",
   "metadata": {},
   "source": [
    "# Logistic Regression Class\n",
    "\n",
    "## Logistic Regression Algorithm\n",
    "\n",
    "### 1. Initialization\n",
    "- **Class Definition**: `LogisticRegression`\n",
    "  - **`__init__` Method**: Initializes the logistic regression model with a specified learning rate and number of iterations.\n",
    "    - **Parameters**:\n",
    "      - `learning_rate`: The step size for gradient descent updates.\n",
    "      - `num_iterations`: The number of iterations for the gradient descent optimization.\n",
    "\n",
    "### 2. Sigmoid Function\n",
    "- **`sigmoid` Method**: Computes the sigmoid function, which maps any real-valued number into the range (0, 1).\n",
    "  - **Formula**: \n",
    "    $$\n",
    "    \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "    $$\n",
    "\n",
    "### 3. Training the Model\n",
    "- **`fit` Method**: Trains the logistic regression model using gradient descent.\n",
    "  - **Parameters**:\n",
    "    - `X`: Feature matrix.\n",
    "    - `y`: Target vector.\n",
    "  - **Process**:\n",
    "    1. Initialize weights and bias to zero.\n",
    "    2. Iterate for the specified number of iterations:\n",
    "       - Compute the linear combination of inputs and weights: $ z = X \\cdot \\text{weights} + \\text{bias} $.\n",
    "       - Apply the sigmoid function to the linear combination to get the predicted probabilities: $ \\hat{y} = \\sigma(z) $.\n",
    "       - Compute the gradients of the cost function with respect to the weights and bias:\n",
    "         - $ \\frac{\\partial J}{\\partial \\text{weights}} = \\frac{1}{m} \\sum ( \\hat{y} - y ) \\cdot X $\n",
    "         - $ \\frac{\\partial J}{\\partial \\text{bias}} = \\frac{1}{m} \\sum ( \\hat{y} - y ) $\n",
    "       - Update the weights and bias using the gradients and the learning rate:\n",
    "         - `weights -= learning_rate * dw`\n",
    "         - `bias -= learning_rate * db`\n",
    "\n",
    "### 4. Making Predictions\n",
    "- **`predict` Method**: Predicts binary labels for input data based on the learned weights and bias.\n",
    "  - **Parameters**:\n",
    "    - `X`: Feature matrix for which predictions are to be made.\n",
    "  - **Process**:\n",
    "    1. Compute the linear combination of inputs and weights: $ z = X \\cdot \\text{weights} + \\text{bias} $.\n",
    "    2. Apply the sigmoid function to the linear combination to get the predicted probabilities: $ \\hat{y} = \\sigma(z) $.\n",
    "    3. Convert the predicted probabilities to binary labels (0 or 1) based on a threshold of 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b48386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500) \n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.m, self.n = X.shape\n",
    "        self.weights = np.zeros(self.n)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.num_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            dw = (1 / self.m) * np.dot(X.T, (y_predicted - y))\n",
    "            db = (1 / self.m) * np.sum(y_predicted - y)\n",
    "\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.sigmoid(linear_model)\n",
    "        return [1 if i > 0.5 else 0 for i in y_predicted]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1b1d",
   "metadata": {},
   "source": [
    "# Function to calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8087f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    return correct / len(y_true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc3379",
   "metadata": {},
   "source": [
    "# Calculating the Accuracy of our custum Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4520b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data1:\n",
      "Training Accuracy for data1_train.csv: 50.00%\n",
      "Test Accuracy for data1_test.csv: 56.50%\n",
      "\n",
      "Results for data2:\n",
      "Training Accuracy for data1_train.csv: 98.88%\n",
      "Test Accuracy for data1_test.csv: 97.00%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_logistic_regression(X_train, y_train, X_test, y_test):\n",
    "    X_train = feature_scaling(X_train)\n",
    "    X_test = feature_scaling(X_test)\n",
    "\n",
    "\n",
    "    model = LogisticRegression(learning_rate=0.01, num_iterations=10000)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = model.predict(X_train)\n",
    "\n",
    "\n",
    "    y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "    train_accuracy = accuracy(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "    print(f\"Training Accuracy for data1_train.csv: {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy for data1_test.csv: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "    \n",
    "train_data = load_data('data1_train.csv')                               # Load and preprocess the first dataset\n",
    "test_data = load_data('data1_test.csv')\n",
    "X_train1, y_train1 = preprocess_data(train_data)\n",
    "X_test1, y_test1 = preprocess_data(test_data)\n",
    "\n",
    "\n",
    "train_data2 = load_data('data2_train.csv')                              # Load and preprocess the second dataset\n",
    "test_data2 = load_data('data2_test.csv')\n",
    "X_train2, y_train2 = preprocess_data(train_data2)\n",
    "X_test2, y_test2 = preprocess_data(test_data2)\n",
    "\n",
    "\n",
    "print(\"Results for data1:\")                                            # Evaluate on the first dataset\n",
    "evaluate_logistic_regression(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nResults for data2:\")                                             # Evaluate on the second dataset\n",
    "evaluate_logistic_regression(X_train2, y_train2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea469824",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "### Introduction\n",
    "Hyperparameter tuning is the process of optimizing the hyperparameters of a model to improve its performance. For logistic regression, common hyperparameters include the learning rate and the number of iterations. The goal is to find the best combination of these hyperparameters that maximizes the model's accuracy.\n",
    "\n",
    "### Grid Search Method\n",
    "Grid search is a systematic method for hyperparameter tuning. It involves training the model with different combinations of hyperparameters and evaluating their performance. The combination that yields the highest accuracy is chosen as the best.\n",
    "\n",
    "Here is the markdown explanation for the hyperparameter tuning process:\n",
    "\n",
    "\n",
    "#### Parameters\n",
    "- `X_train`: Feature matrix for the training data.\n",
    "- `y_train`: Target vector for the training data.\n",
    "- `learning_rates`: List of learning rates to try.\n",
    "- `num_iterations_list`: List of iteration counts to try.\n",
    "\n",
    "#### Process\n",
    "1. **Initialize Best Accuracy and Parameters:**\n",
    "   - `best_accuracy` is initialized to 0.\n",
    "   - `best_params` is initialized as an empty dictionary.\n",
    "   \n",
    "2. **Iterate Over Hyperparameter Combinations:**\n",
    "   - For each learning rate in `learning_rates`:\n",
    "     - For each number of iterations in `num_iterations_list`:\n",
    "       - Create a new instance of the `LogisticRegression` model with the current learning rate and number of iterations.\n",
    "       - Fit the model to the training data.\n",
    "       - Predict the labels for the training data.\n",
    "       - Calculate the accuracy of the model on the training data.\n",
    "       - If the current accuracy is higher than `best_accuracy`, update `best_accuracy` and `best_params`.\n",
    "\n",
    "3. **Return Best Parameters and Accuracy:**\n",
    "   - After iterating through all combinations, return the `best_params` and `best_accuracy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd4d338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data1:\n",
      "Best Parameters: {'learning_rate': 0.001, 'num_iterations': 100}\n",
      "Best Training Accuracy: 32.25%\n",
      "\n",
      "Results for data2:\n",
      "Best Parameters: {'learning_rate': 0.001, 'num_iterations': 1000}\n",
      "Best Training Accuracy: 89.38%\n"
     ]
    }
   ],
   "source": [
    "def grid_search(X_train, y_train, learning_rates, num_iterations_list):\n",
    "    best_accuracy = 0\n",
    "    best_params = {}\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for num_iter in num_iterations_list:\n",
    "            model = LogisticRegression(learning_rate=lr, num_iterations=num_iter)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_train)\n",
    "            acc = accuracy(y_train, y_pred)\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_params = {'learning_rate': lr, 'num_iterations': num_iter}\n",
    "    \n",
    "    return best_params, best_accuracy\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_hyperparameter(X_train, y_train, X_test, y_test):\n",
    "    learning_rates = [0.001, 0.01, 0.1]\n",
    "    num_iterations_list = [100, 500, 1000]\n",
    "    best_params, best_accuracy = grid_search(X_train, y_train, learning_rates, num_iterations_list)\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best Training Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "\n",
    "train_data = load_data('data1_train.csv')                               # Load and preprocess the first dataset\n",
    "test_data = load_data('data1_test.csv')\n",
    "X_train1, y_train1 = preprocess_data(train_data)\n",
    "X_test1, y_test1 = preprocess_data(test_data)\n",
    "\n",
    "\n",
    "train_data2 = load_data('data2_train.csv')                              # Load and preprocess the second dataset\n",
    "test_data2 = load_data('data2_test.csv')\n",
    "X_train2, y_train2 = preprocess_data(train_data2)\n",
    "X_test2, y_test2 = preprocess_data(test_data2)\n",
    "\n",
    "\n",
    "print(\"Results for data1:\")                                            # Evaluate on the first dataset\n",
    "evaluate_hyperparameter(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nResults for data2:\")                                             # Evaluate on the second dataset\n",
    "evaluate_hyperparameter(X_train2, y_train2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40575b",
   "metadata": {},
   "source": [
    "#  Comparison with Scikit-Learn\n",
    "\n",
    "### `evaluate_sklearn` Function Explanation\n",
    "\n",
    "The `evaluate_sklearn` function in your code performs the following steps to train and evaluate a logistic regression model using scikit-learn:\n",
    "\n",
    "1. **Data Scaling**:\n",
    "   - **Standardization**: The function begins by scaling the features of both the training and test datasets using a `StandardScaler`. This ensures that each feature has a mean of 0 and a standard deviation of 1. Standardization is important because it ensures that the model treats all features equally and helps in the convergence of the algorithm.\n",
    "\n",
    "2. **Model Initialization**:\n",
    "   - A `LogisticRegression` model is initialized. In this case, the `lbfgs` solver is used, which is efficient for small to medium-sized datasets. The maximum number of iterations is set to a high value (10,000) to ensure that the optimization process converges.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - The logistic regression model is trained on the scaled training data. During training, the model learns the coefficients that best fit the relationship between the input features and the target labels by minimizing the logistic loss function.\n",
    "\n",
    "4. **Prediction**:\n",
    "   - After training, the model makes predictions on both the training and test datasets. This involves calculating the probability that each sample belongs to a particular class and then assigning the sample to the class with the highest probability.\n",
    "\n",
    "5. **Evaluation**:\n",
    "   - The function evaluates the model's performance by calculating the accuracy on the training and test sets. Accuracy is the ratio of correctly predicted instances to the total instances:\n",
    "     $$\n",
    "     \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "     $$\n",
    "   - The function then prints the accuracy for both the training and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18f5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data1:\n",
      "Training Accuracy for : 97.25%\n",
      "Test Accuracy for : 98.00%\n",
      "\n",
      "Results for data2:\n",
      "Training Accuracy for : 98.88%\n",
      "Test Accuracy for : 99.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def evaluate_sklearn(X_train, y_train, X_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "    model = LogisticRegression(solver='lbfgs', max_iter=10000)\n",
    "\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "\n",
    "\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Training Accuracy for : {train_accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Accuracy for : {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "train_data = load_data('data1_train.csv')                               # Load and preprocess the first dataset\n",
    "test_data = load_data('data1_test.csv')\n",
    "X_train1, y_train1 = preprocess_data(train_data)\n",
    "X_test1, y_test1 = preprocess_data(test_data)\n",
    "\n",
    "\n",
    "train_data2 = load_data('data2_train.csv')                              # Load and preprocess the second dataset\n",
    "test_data2 = load_data('data2_test.csv')\n",
    "X_train2, y_train2 = preprocess_data(train_data2)\n",
    "X_test2, y_test2 = preprocess_data(test_data2)\n",
    "\n",
    "\n",
    "print(\"Results for data1:\")                                            # Evaluate on the first dataset\n",
    "evaluate_sklearn(X_train1, y_train1, X_test1, y_test1)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nResults for data2:\")                                             # Evaluate on the second dataset\n",
    "evaluate_sklearn(X_train2, y_train2, X_test2, y_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b51dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
